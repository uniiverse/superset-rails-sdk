=begin
#Superset

#Superset

OpenAPI spec version: v1

Generated by: https://github.com/swagger-api/swagger-codegen.git
Swagger Codegen version: 3.0.40
=end

require 'date'

module SwaggerClient
  class ChartDataQueryObject
    # Annotation layers to apply to chart
    attr_accessor :annotation_layers

    # A mapping of temporal extras that have been applied to the query
    attr_accessor :applied_time_extras

    # Add fetch values predicate (where clause) to query if defined in datasource
    attr_accessor :apply_fetch_values_predicate

    # Columns which to select in the query.
    attr_accessor :columns

    attr_accessor :datasource

    # Starting point for time grain counting on legacy Druid datasources. Used to change e.g. Monday/Sunday first-day-of-week. This field is deprecated and should be passed to `extras` as `druid_time_origin`.
    attr_accessor :druid_time_origin

    # Extra parameters to add to the query.
    attr_accessor :extras

    attr_accessor :filters

    # Name of temporal column used for time filtering. For legacy Druid datasources this defines the time grain.
    attr_accessor :granularity

    # Name of temporal column used for time filtering for SQL datasources. This field is deprecated, use `granularity` instead.
    attr_accessor :granularity_sqla

    # Columns by which to group the query. This field is deprecated, use `columns` instead.
    attr_accessor :groupby

    # HAVING clause to be added to aggregate queries using AND operator. This field is deprecated and should be passed to `extras`.
    attr_accessor :having

    # HAVING filters to be added to legacy Druid datasource queries. This field is deprecated and should be passed to `extras` as `having_druid`.
    attr_accessor :having_filters

    # Should the rowcount of the actual query be returned
    attr_accessor :is_rowcount

    # Is the `query_object` a timeseries.
    attr_accessor :is_timeseries

    # Aggregate expressions. Metrics can be passed as both references to datasource metrics (strings), or ad-hoc metricswhich are defined only within the query object. See `ChartDataAdhocMetricSchema` for the structure of ad-hoc metrics.
    attr_accessor :metrics

    # Reverse order. Default: `false`
    attr_accessor :order_desc

    # Expects a list of lists where the first element is the column name which to sort by, and the second element is a boolean.
    attr_accessor :orderby

    # Post processing operations to be applied to the result set. Operations are applied to the result set in sequential order.
    attr_accessor :post_processing

    attr_accessor :result_type

    # Maximum row count (0=disabled). Default: `config[\"ROW_LIMIT\"]`
    attr_accessor :row_limit

    # Number of rows to skip. Default: `0`
    attr_accessor :row_offset

    # Columns to use when limiting series count. All columns must be present in the `columns` property. Requires `series_limit` and `series_limit_metric` to be set.
    attr_accessor :series_columns

    # Maximum number of series. Requires `series` and `series_limit_metric` to be set.
    attr_accessor :series_limit

    # Metric used to limit timeseries queries by. Requires `series` and `series_limit` to be set.
    attr_accessor :series_limit_metric

    attr_accessor :time_offsets

    # A time rage, either expressed as a colon separated string `since : until` or human readable freeform. Valid formats for `since` and `until` are:  - ISO 8601 - X days/years/hours/day/year/weeks - X days/years/hours/day/year/weeks ago - X days/years/hours/day/year/weeks from now  Additionally, the following freeform can be used:  - Last day - Last week - Last month - Last quarter - Last year - No filter - Last X seconds/minutes/hours/days/weeks/months/years - Next X seconds/minutes/hours/days/weeks/months/years 
    attr_accessor :time_range

    # A human-readable date/time string. Please refer to [parsdatetime](https://github.com/bear/parsedatetime) documentation for details on valid values.
    attr_accessor :time_shift

    # Maximum row count for timeseries queries. This field is deprecated, use `series_limit` instead.Default: `0`
    attr_accessor :timeseries_limit

    # Metric used to limit timeseries queries by. This field is deprecated, use `series_limit_metric` instead.
    attr_accessor :timeseries_limit_metric

    # Optional query parameters passed to a dashboard or Explore view
    attr_accessor :url_params

    # WHERE clause to be added to queries using AND operator.This field is deprecated and should be passed to `extras`.
    attr_accessor :where

    # Attribute mapping from ruby-style variable name to JSON key.
    def self.attribute_map
      {
        :'annotation_layers' => :'annotation_layers',
        :'applied_time_extras' => :'applied_time_extras',
        :'apply_fetch_values_predicate' => :'apply_fetch_values_predicate',
        :'columns' => :'columns',
        :'datasource' => :'datasource',
        :'druid_time_origin' => :'druid_time_origin',
        :'extras' => :'extras',
        :'filters' => :'filters',
        :'granularity' => :'granularity',
        :'granularity_sqla' => :'granularity_sqla',
        :'groupby' => :'groupby',
        :'having' => :'having',
        :'having_filters' => :'having_filters',
        :'is_rowcount' => :'is_rowcount',
        :'is_timeseries' => :'is_timeseries',
        :'metrics' => :'metrics',
        :'order_desc' => :'order_desc',
        :'orderby' => :'orderby',
        :'post_processing' => :'post_processing',
        :'result_type' => :'result_type',
        :'row_limit' => :'row_limit',
        :'row_offset' => :'row_offset',
        :'series_columns' => :'series_columns',
        :'series_limit' => :'series_limit',
        :'series_limit_metric' => :'series_limit_metric',
        :'time_offsets' => :'time_offsets',
        :'time_range' => :'time_range',
        :'time_shift' => :'time_shift',
        :'timeseries_limit' => :'timeseries_limit',
        :'timeseries_limit_metric' => :'timeseries_limit_metric',
        :'url_params' => :'url_params',
        :'where' => :'where'
      }
    end

    # Attribute type mapping.
    def self.openapi_types
      {
        :'annotation_layers' => :'Object',
        :'applied_time_extras' => :'Object',
        :'apply_fetch_values_predicate' => :'Object',
        :'columns' => :'Object',
        :'datasource' => :'Object',
        :'druid_time_origin' => :'Object',
        :'extras' => :'Object',
        :'filters' => :'Object',
        :'granularity' => :'Object',
        :'granularity_sqla' => :'Object',
        :'groupby' => :'Object',
        :'having' => :'Object',
        :'having_filters' => :'Object',
        :'is_rowcount' => :'Object',
        :'is_timeseries' => :'Object',
        :'metrics' => :'Object',
        :'order_desc' => :'Object',
        :'orderby' => :'Object',
        :'post_processing' => :'Object',
        :'result_type' => :'Object',
        :'row_limit' => :'Object',
        :'row_offset' => :'Object',
        :'series_columns' => :'Object',
        :'series_limit' => :'Object',
        :'series_limit_metric' => :'Object',
        :'time_offsets' => :'Object',
        :'time_range' => :'Object',
        :'time_shift' => :'Object',
        :'timeseries_limit' => :'Object',
        :'timeseries_limit_metric' => :'Object',
        :'url_params' => :'Object',
        :'where' => :'Object'
      }
    end

    # List of attributes with nullable: true
    def self.openapi_nullable
      Set.new([
        :'annotation_layers',
        :'applied_time_extras',
        :'apply_fetch_values_predicate',
        :'columns',
        :'datasource',
        :'druid_time_origin',
        :'extras',
        :'filters',
        :'granularity',
        :'granularity_sqla',
        :'groupby',
        :'having',
        :'having_filters',
        :'is_rowcount',
        :'is_timeseries',
        :'metrics',
        :'order_desc',
        :'orderby',
        :'post_processing',
        :'result_type',
        :'row_limit',
        :'row_offset',
        :'series_columns',
        :'series_limit',
        :'series_limit_metric',
        :'time_offsets',
        :'time_range',
        :'time_shift',
        :'timeseries_limit',
        :'timeseries_limit_metric',
        :'url_params',
        :'where'
      ])
    end
  
    # Initializes the object
    # @param [Hash] attributes Model attributes in the form of hash
    def initialize(attributes = {})
      if (!attributes.is_a?(Hash))
        fail ArgumentError, "The input argument (attributes) must be a hash in `SwaggerClient::ChartDataQueryObject` initialize method"
      end

      # check to see if the attribute exists and convert string to symbol for hash key
      attributes = attributes.each_with_object({}) { |(k, v), h|
        if (!self.class.attribute_map.key?(k.to_sym))
          fail ArgumentError, "`#{k}` is not a valid attribute in `SwaggerClient::ChartDataQueryObject`. Please check the name to make sure it's valid. List of attributes: " + self.class.attribute_map.keys.inspect
        end
        h[k.to_sym] = v
      }

      if attributes.key?(:'annotation_layers')
        if (value = attributes[:'annotation_layers']).is_a?(Array)
          self.annotation_layers = value
        end
      end

      if attributes.key?(:'applied_time_extras')
        self.applied_time_extras = attributes[:'applied_time_extras']
      end

      if attributes.key?(:'apply_fetch_values_predicate')
        self.apply_fetch_values_predicate = attributes[:'apply_fetch_values_predicate']
      end

      if attributes.key?(:'columns')
        if (value = attributes[:'columns']).is_a?(Array)
          self.columns = value
        end
      end

      if attributes.key?(:'datasource')
        self.datasource = attributes[:'datasource']
      end

      if attributes.key?(:'druid_time_origin')
        self.druid_time_origin = attributes[:'druid_time_origin']
      end

      if attributes.key?(:'extras')
        self.extras = attributes[:'extras']
      end

      if attributes.key?(:'filters')
        if (value = attributes[:'filters']).is_a?(Array)
          self.filters = value
        end
      end

      if attributes.key?(:'granularity')
        self.granularity = attributes[:'granularity']
      end

      if attributes.key?(:'granularity_sqla')
        self.granularity_sqla = attributes[:'granularity_sqla']
      end

      if attributes.key?(:'groupby')
        if (value = attributes[:'groupby']).is_a?(Array)
          self.groupby = value
        end
      end

      if attributes.key?(:'having')
        self.having = attributes[:'having']
      end

      if attributes.key?(:'having_filters')
        if (value = attributes[:'having_filters']).is_a?(Array)
          self.having_filters = value
        end
      end

      if attributes.key?(:'is_rowcount')
        self.is_rowcount = attributes[:'is_rowcount']
      end

      if attributes.key?(:'is_timeseries')
        self.is_timeseries = attributes[:'is_timeseries']
      end

      if attributes.key?(:'metrics')
        if (value = attributes[:'metrics']).is_a?(Array)
          self.metrics = value
        end
      end

      if attributes.key?(:'order_desc')
        self.order_desc = attributes[:'order_desc']
      end

      if attributes.key?(:'orderby')
        if (value = attributes[:'orderby']).is_a?(Array)
          self.orderby = value
        end
      end

      if attributes.key?(:'post_processing')
        if (value = attributes[:'post_processing']).is_a?(Array)
          self.post_processing = value
        end
      end

      if attributes.key?(:'result_type')
        self.result_type = attributes[:'result_type']
      end

      if attributes.key?(:'row_limit')
        self.row_limit = attributes[:'row_limit']
      end

      if attributes.key?(:'row_offset')
        self.row_offset = attributes[:'row_offset']
      end

      if attributes.key?(:'series_columns')
        if (value = attributes[:'series_columns']).is_a?(Array)
          self.series_columns = value
        end
      end

      if attributes.key?(:'series_limit')
        self.series_limit = attributes[:'series_limit']
      end

      if attributes.key?(:'series_limit_metric')
        self.series_limit_metric = attributes[:'series_limit_metric']
      end

      if attributes.key?(:'time_offsets')
        if (value = attributes[:'time_offsets']).is_a?(Array)
          self.time_offsets = value
        end
      end

      if attributes.key?(:'time_range')
        self.time_range = attributes[:'time_range']
      end

      if attributes.key?(:'time_shift')
        self.time_shift = attributes[:'time_shift']
      end

      if attributes.key?(:'timeseries_limit')
        self.timeseries_limit = attributes[:'timeseries_limit']
      end

      if attributes.key?(:'timeseries_limit_metric')
        self.timeseries_limit_metric = attributes[:'timeseries_limit_metric']
      end

      if attributes.key?(:'url_params')
        if (value = attributes[:'url_params']).is_a?(Hash)
          self.url_params = value
        end
      end

      if attributes.key?(:'where')
        self.where = attributes[:'where']
      end
    end

    # Show invalid properties with the reasons. Usually used together with valid?
    # @return Array for valid properties with the reasons
    def list_invalid_properties
      invalid_properties = Array.new
      invalid_properties
    end

    # Check to see if the all the properties in the model are valid
    # @return true if the model is valid
    def valid?
      true
    end

    # Checks equality by comparing each attribute.
    # @param [Object] Object to be compared
    def ==(o)
      return true if self.equal?(o)
      self.class == o.class &&
          annotation_layers == o.annotation_layers &&
          applied_time_extras == o.applied_time_extras &&
          apply_fetch_values_predicate == o.apply_fetch_values_predicate &&
          columns == o.columns &&
          datasource == o.datasource &&
          druid_time_origin == o.druid_time_origin &&
          extras == o.extras &&
          filters == o.filters &&
          granularity == o.granularity &&
          granularity_sqla == o.granularity_sqla &&
          groupby == o.groupby &&
          having == o.having &&
          having_filters == o.having_filters &&
          is_rowcount == o.is_rowcount &&
          is_timeseries == o.is_timeseries &&
          metrics == o.metrics &&
          order_desc == o.order_desc &&
          orderby == o.orderby &&
          post_processing == o.post_processing &&
          result_type == o.result_type &&
          row_limit == o.row_limit &&
          row_offset == o.row_offset &&
          series_columns == o.series_columns &&
          series_limit == o.series_limit &&
          series_limit_metric == o.series_limit_metric &&
          time_offsets == o.time_offsets &&
          time_range == o.time_range &&
          time_shift == o.time_shift &&
          timeseries_limit == o.timeseries_limit &&
          timeseries_limit_metric == o.timeseries_limit_metric &&
          url_params == o.url_params &&
          where == o.where
    end

    # @see the `==` method
    # @param [Object] Object to be compared
    def eql?(o)
      self == o
    end

    # Calculates hash code according to all attributes.
    # @return [Integer] Hash code
    def hash
      [annotation_layers, applied_time_extras, apply_fetch_values_predicate, columns, datasource, druid_time_origin, extras, filters, granularity, granularity_sqla, groupby, having, having_filters, is_rowcount, is_timeseries, metrics, order_desc, orderby, post_processing, result_type, row_limit, row_offset, series_columns, series_limit, series_limit_metric, time_offsets, time_range, time_shift, timeseries_limit, timeseries_limit_metric, url_params, where].hash
    end

    # Builds the object from hash
    # @param [Hash] attributes Model attributes in the form of hash
    # @return [Object] Returns the model itself
    def self.build_from_hash(attributes)
      new.build_from_hash(attributes)
    end

    # Builds the object from hash
    # @param [Hash] attributes Model attributes in the form of hash
    # @return [Object] Returns the model itself
    def build_from_hash(attributes)
      return nil unless attributes.is_a?(Hash)
      self.class.openapi_types.each_pair do |key, type|
        if type =~ /\AArray<(.*)>/i
          # check to ensure the input is an array given that the attribute
          # is documented as an array but the input is not
          if attributes[self.class.attribute_map[key]].is_a?(Array)
            self.send("#{key}=", attributes[self.class.attribute_map[key]].map { |v| _deserialize($1, v) })
          end
        elsif !attributes[self.class.attribute_map[key]].nil?
          self.send("#{key}=", _deserialize(type, attributes[self.class.attribute_map[key]]))
        elsif attributes[self.class.attribute_map[key]].nil? && self.class.openapi_nullable.include?(key)
          self.send("#{key}=", nil)
        end
      end

      self
    end

    # Deserializes the data based on type
    # @param string type Data type
    # @param string value Value to be deserialized
    # @return [Object] Deserialized data
    def _deserialize(type, value)
      case type.to_sym
      when :DateTime
        DateTime.parse(value)
      when :Date
        Date.parse(value)
      when :String
        value.to_s
      when :Integer
        value.to_i
      when :Float
        value.to_f
      when :Boolean
        if value.to_s =~ /\A(true|t|yes|y|1)\z/i
          true
        else
          false
        end
      when :Object
        # generic object (usually a Hash), return directly
        value
      when /\AArray<(?<inner_type>.+)>\z/
        inner_type = Regexp.last_match[:inner_type]
        value.map { |v| _deserialize(inner_type, v) }
      when /\AHash<(?<k_type>.+?), (?<v_type>.+)>\z/
        k_type = Regexp.last_match[:k_type]
        v_type = Regexp.last_match[:v_type]
        {}.tap do |hash|
          value.each do |k, v|
            hash[_deserialize(k_type, k)] = _deserialize(v_type, v)
          end
        end
      else # model
        SwaggerClient.const_get(type).build_from_hash(value)
      end
    end

    # Returns the string representation of the object
    # @return [String] String presentation of the object
    def to_s
      to_hash.to_s
    end

    # to_body is an alias to to_hash (backward compatibility)
    # @return [Hash] Returns the object in the form of hash
    def to_body
      to_hash
    end

    # Returns the object in the form of hash
    # @return [Hash] Returns the object in the form of hash
    def to_hash
      hash = {}
      self.class.attribute_map.each_pair do |attr, param|
        value = self.send(attr)
        if value.nil?
          is_nullable = self.class.openapi_nullable.include?(attr)
          next if !is_nullable || (is_nullable && !instance_variable_defined?(:"@#{attr}"))
        end

        hash[param] = _to_hash(value)
      end
      hash
    end

    # Outputs non-array value in the form of hash
    # For object, use to_hash. Otherwise, just return the value
    # @param [Object] value Any valid value
    # @return [Hash] Returns the value in the form of hash
    def _to_hash(value)
      if value.is_a?(Array)
        value.compact.map { |v| _to_hash(v) }
      elsif value.is_a?(Hash)
        {}.tap do |hash|
          value.each { |k, v| hash[k] = _to_hash(v) }
        end
      elsif value.respond_to? :to_hash
        value.to_hash
      else
        value
      end
    end  end
end
